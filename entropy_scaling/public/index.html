<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>A connection between Discrete and Continuous Entropy</title>
    <script src="https://distill.pub/template.v2.js"></script>
    <script src="https://d3js.org/d3.v5.min.js"></script>

    <!-- <script defer src="js/main.js"></script>
     <script defer src="js/bundle.js"></script>
     <link rel="stylesheet" type="text/css" href="css/style.css">
    -->

</head>
<body>

<script type="text/bibliography">
  @article{gregor2015draw,
    title={DRAW: A recurrent neural network for image generation},
    author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
    journal={arXivreprint arXiv:1502.04623},
    year={2015},
    url={https://arxiv.org/pdf/1502.04623.pdf}
  }
</script>

<d-front-matter>
    <script type="text/json">{
  "title": "A connection between discrete and continuous entropy",
  "description": "A short exploration of scale and shape, and their effect on entropy",
  "authors": [
  {
      "author": "Henry Bigelow"
  }
  ]
  }</script>
</d-front-matter>

<d-title style="padding-bottom: 0">
    <h1>Equivalence of entropies of discrete and continuous distributions</h1>
</d-title>

<d-byline></d-byline>


<d-article>
  <h3>I give a simple derivation that shows the equivalence of entropies of
      discrete and continuous distributions.  The distributions can be
      understood in terms of shape and domain size, with corresponding terms in
  the entropy for each.</h3>

  <figure>
      <d-figure>
          <svg class="densityplot"></svg>
          <figcaption><b>P(x), a piecewise-constant density</b></figcaption>
      </d-figure>
  </figure>
  <figure>
      <d-figure>
        <svg class="barchart"></svg>
        <figcaption><b>M(i), a discrete probability mass
                function</b></figcaption>
      </d-figure>
  </figure>

  <p>Shown above are two probability distributions with the same <i>shape</i>.
  The first, <d-math>P(x)</d-math> is a peculiar looking, piecewise-constant
  density, defined on the <i>real</i> number interval <d-math>x: [0,
  n]</d-math> for some <i>integer</i> <d-math>n</d-math>.  The second,
  <d-math>M(i)</d-math> is a discrete distribution defined on the integers
  <d-math>i: [0, n-1]</d-math>.  They are the same shape in the sense that
  <d-math>P(i) = M(i)\, \forall i \in dom(M)</d-math>.  We then have:

  <d-math block>
      \begin{aligned}
      - H(P) &\equiv \int_0^n{dx\, P(x) \log P(x)} \\
      &= \sum_{i=0}^{n-1}{\int_i^{i+1}{dx\, P(x) \log P(x)}} \\
      &= \sum_{i=0}^{n-1}{P(i) \log P(i)} \\
      &= \sum_{i=0}^{n-1}{M(i) \log M(i)} \\
      &= -H(M)
      \end{aligned}
  </d-math>

  <p>Breaking up the integral into unit-sized intervals with constant value, 
  then solving each one, we see that the entropies are the same.  Putting this
  aside now, what happens when we compare <d-math>P(x)</d-math> to a
  distribution of the same shape, but defined on the size 1 domain <d-math>[0,
      1]</d-math>?

  <d-math block>
      \begin{aligned}
      Q(t) &\equiv n P(nt) \\
      -H(Q) &\equiv \int_0^1{dt\, Q(t) \log Q(t)} \\
      &= \int_0^1{ dt\, n P(nt) \log [n P(nt)]} \\
      &= \int_0^n{ \frac{dx}{n}\, n P(x) [\log n + \log P(x)]} \\
      &= \int_0^n{ dx\, P(x) \log n} + \int_0^n{dx\, P(x) \log P(x)} \\
      &= \log n - H(P) \\
      \\
      H(Q) &= H(P) - \log n
      \end{aligned}
  </d-math>

  <p>We see that it differs by <d-math>\log n</d-math>, which happens to be the
  domain size of <d-math>P</d-math>.  Also, <d-math>\log n</d-math> happens to
  be the entropy of the uniform distribution on <d-math>[0, n]</d-math> or any
  domain of size <d-math>n</d-math> for continuous distributions, or of
  <d-math>n</d-math> categories for discrete distributions.</p>

  <p>This is a nice result. It says that we can interpret discrete
  distributions over <d-math>n</d-math> categories as piecewise constant
  continuous distributions over a domain of size <d-math>n</d-math>, and the
  entropy will be the same.  Secondly, it shows that the entropy of a
  distribution can be conceptualized into two parts: a <i>shape</i> component
  and a <i>domain size</i> component.</p>

  <p>Finally, this implies that if we have two distributions defined on the
  same domain, the difference of their entropies only reflects the difference
  in shape of the two distributions.  Domain size does not play a role.</p>

  <p>The shape + size idea also applies to joint entropies <d-math>H(P,
      Q)</d-math> and conditional entropies <d-math>H(P|Q)</d-math>.  Because of
  this, KL-divergence is also domain-size invariant, because it is a difference
  of entropies over the same domain:</p>

  <d-math>
      \begin{aligned}
      D_{KL}(P||Q) &\equiv \int{dx\, P(x) \log ({P(x) \over Q(x)})} \\
      &= H(P,Q) - H(P)
      \end{aligned}
  </d-math>

  <p>Another example is mutual information, defined on a two variable joint
  distribution:</p>

  <d-math>
      \begin{aligned}
      I(A; B) &\equiv D_{KL}(P(A, B) || P(A)P(B)) \\
      &= H(A) - H(A|B) \\
      &= H(B) - H(B|A)
      \end{aligned}
  </d-math>

  <p>The first difference above is a difference of entropies over the domain
  <d-math>A</d-math>, and the second is a difference over <d-math>B</d-math>.
  Even though the domain sizes may be different, both differences are
  domain-size invariant, and turn out to be equivalent, which makes mutual
  information between two variables a symmetric measure.</p>

</d-article>

<d-appendix>
</d-appendix>

<script>
    var chartHeight = 250,
        chartWidth = 650,
        margin = {top: 20, right: 20, bottom: 30, left: 20};

    var x = d3.scaleBand()
        .padding(0.1);

    var xd = d3.scaleLinear()

    var y = d3.scaleLinear()
        .range([chartHeight - margin.bottom, margin.top]);

    var barChart = d3.select(".barchart")
        .attr("height", chartHeight);

    var densityPlot = d3.select(".densityplot")
        .attr("height", chartHeight);

    function xval(maxval, maxvalstring) {
        return function(i) {
            if (i < 10) return i;
            else if (i == maxval) return maxvalstring;
            else return '.';
        }
    }

    function xAxis(g, scale, minval, maxval, maxvalstring) { 
        yc = chartHeight - margin.bottom;
        g.attr("transform", (d, i) => `translate(0, ${yc})`)
        .call(d3.axisBottom(scale).tickValues(d3.range(minval, maxval + 1))
            .tickFormat(xval(maxval, maxvalstring)).tickSizeOuter(0))
    }

    function makeDensityPlot(data) {
        barWidth = (chartWidth - margin.left - margin.right) / data.length;
        console.log(`data.length = ${data.length}`);
        xd.domain([0, data.length])
            .range([margin.left, chartWidth - margin.right]);
        y.domain([0, d3.max(data, function(d) { return d.value; })]);
        console.log(`xd(1): ${xd(1)}, xd(2): ${xd(2)}`);

        densityPlot.attr("width", chartWidth);
        
        var segment = densityPlot.selectAll("g")
            .data(data)
            .enter()
            .append("g");

        segment.append("path")
            .attr("d", (d, i) => `M${xd(i)} ${y(d.value)} h${xd(1) - xd(0)} z`)
            .attr("stroke", "black")
            .attr("stroke-width", 2)
            .attr("fill", "transparent");
        segment.append("circle")
            .attr("cx", (d, i) => xd(i+1))
            .attr("cy", (d, i) => y(d.value))
            .attr("r", 3)
            .attr("stroke", "black")
            .attr("stroke-width", 2)
            .attr("fill", "white");
            
        densityPlot.append("g")
            .style("font-size", "12px")
            .call(xAxis, xd, 0, data.length, 'n');

    }

    function makeBarChart(data) {
        barWidth = (chartWidth - margin.left - margin.right) / data.length;
        x.domain(d3.range(0, data.length))
            .range([margin.left, chartWidth - margin.right]);

        y.domain([0, d3.max(data, function(d) { return d.value; })]);

        // console.log('data: ' + JSON.stringify(data))
        barChart.attr("width", chartWidth);

        var bar = barChart.selectAll("g")
            .data(data)
            .enter()
            .append("g");

        bar.append("rect")
            .attr("x", (d, i) => x(i))
            .attr("y", d => y(d.value))
            .attr("width", x.bandwidth())
            .attr("height", d => y(0) - y(d.value));

        barChart.append("g")
            .style("font-size", "12px")
            .call(xAxis, x, 0, data.length - 1, 'n-1');
    }

    function convType(d) {
      d.value = +d.value; // coerce to number
      // console.log('type: ' + d.value)
      return d;
    }

    async function populate(file) {
        data = await d3.tsv(file, convType);
        makeBarChart(data);
        makeDensityPlot(data);
        // console.log(JSON.stringify(res))
    }

    populate("data.tsv");

</script>

<style>
.chart rect {
    fill: gray;
}
figcaption {
    text-align: center;
}
</style>

</body>
</html>

